{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72077926",
   "metadata": {},
   "source": [
    "## Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e66a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7833dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 00:22:21.343942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744147341.355839   11579 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744147341.359300   11579 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744147341.369390   11579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744147341.369402   11579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744147341.369404   11579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744147341.369405   11579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 00:22:21.372700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Yelp2018 data\n",
      "user_field_M 60641\n",
      "item_field_M 34858\n",
      "Total fields 95499\n",
      "item_bind_M 32857\n",
      "user_bind_M 60640\n",
      "# of training samples: 547491\n",
      "# of test samples: 60640\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.001\n",
      "WARNING:tensorflow:From /home/leo/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744147350.249116   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Initial evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744147350.513073   11579 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0022, NDCG=0.0014\n",
      "Top 10: HR=0.0054, NDCG=0.0025\n",
      "Top 20: HR=0.0103, NDCG=0.0037\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0022\n",
      "Top 10: HR=0.0075, NDCG=0.0034\n",
      "Top 20: HR=0.0143, NDCG=0.0051\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0022\n",
      "Top 10: HR=0.0076, NDCG=0.0034\n",
      "Top 20: HR=0.0142, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0022\n",
      "Top 10: HR=0.0076, NDCG=0.0034\n",
      "Top 20: HR=0.0141, NDCG=0.0051\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0023\n",
      "Top 10: HR=0.0076, NDCG=0.0035\n",
      "Top 20: HR=0.0142, NDCG=0.0051\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0024\n",
      "Top 10: HR=0.0076, NDCG=0.0035\n",
      "Top 20: HR=0.0142, NDCG=0.0051\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744147918.465809   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0028, NDCG=0.0018\n",
      "Top 10: HR=0.0061, NDCG=0.0028\n",
      "Top 20: HR=0.0117, NDCG=0.0042\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0023\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0078, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0052\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0078, NDCG=0.0036\n",
      "Top 20: HR=0.0144, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0024\n",
      "Top 10: HR=0.0078, NDCG=0.0036\n",
      "Top 20: HR=0.0145, NDCG=0.0053\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744148490.049195   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0029, NDCG=0.0017\n",
      "Top 10: HR=0.0064, NDCG=0.0028\n",
      "Top 20: HR=0.0123, NDCG=0.0043\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0036, NDCG=0.0023\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0052\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744149056.937930   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0036, NDCG=0.0023\n",
      "Top 10: HR=0.0073, NDCG=0.0035\n",
      "Top 20: HR=0.0136, NDCG=0.0051\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0076, NDCG=0.0036\n",
      "Top 20: HR=0.0139, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0052\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0144, NDCG=0.0053\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0026\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0146, NDCG=0.0054\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744149628.589682   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0073, NDCG=0.0035\n",
      "Top 20: HR=0.0136, NDCG=0.0051\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0076, NDCG=0.0036\n",
      "Top 20: HR=0.0140, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0026\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0144, NDCG=0.0054\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0044, NDCG=0.0027\n",
      "Top 10: HR=0.0081, NDCG=0.0038\n",
      "Top 20: HR=0.0148, NDCG=0.0055\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744150191.703763   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0034, NDCG=0.0022\n",
      "Top 10: HR=0.0070, NDCG=0.0034\n",
      "Top 20: HR=0.0138, NDCG=0.0051\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0024\n",
      "Top 10: HR=0.0076, NDCG=0.0036\n",
      "Top 20: HR=0.0140, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0076, NDCG=0.0036\n",
      "Top 20: HR=0.0137, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0042, NDCG=0.0026\n",
      "Top 10: HR=0.0077, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0044, NDCG=0.0027\n",
      "Top 10: HR=0.0079, NDCG=0.0038\n",
      "Top 20: HR=0.0146, NDCG=0.0055\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0048, NDCG=0.0029\n",
      "Top 10: HR=0.0084, NDCG=0.0040\n",
      "Top 20: HR=0.0156, NDCG=0.0058\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.1, negative_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744150757.164738   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0024\n",
      "Top 10: HR=0.0073, NDCG=0.0035\n",
      "Top 20: HR=0.0134, NDCG=0.0050\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0037, NDCG=0.0023\n",
      "Top 10: HR=0.0076, NDCG=0.0036\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0075, NDCG=0.0036\n",
      "Top 20: HR=0.0135, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0140, NDCG=0.0052\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0026\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0045, NDCG=0.0027\n",
      "Top 10: HR=0.0080, NDCG=0.0039\n",
      "Top 20: HR=0.0150, NDCG=0.0056\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744151317.912279   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0034, NDCG=0.0022\n",
      "Top 10: HR=0.0063, NDCG=0.0031\n",
      "Top 20: HR=0.0125, NDCG=0.0047\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0022\n",
      "Top 10: HR=0.0077, NDCG=0.0034\n",
      "Top 20: HR=0.0142, NDCG=0.0051\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0023\n",
      "Top 10: HR=0.0078, NDCG=0.0035\n",
      "Top 20: HR=0.0144, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0025\n",
      "Top 10: HR=0.0080, NDCG=0.0036\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0044, NDCG=0.0025\n",
      "Top 10: HR=0.0077, NDCG=0.0035\n",
      "Top 20: HR=0.0141, NDCG=0.0051\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0036\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744151879.416879   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0036, NDCG=0.0021\n",
      "Top 10: HR=0.0069, NDCG=0.0032\n",
      "Top 20: HR=0.0122, NDCG=0.0045\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0024\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0036\n",
      "Top 20: HR=0.0142, NDCG=0.0052\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0024\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0053\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0145, NDCG=0.0053\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744152439.517273   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0074, NDCG=0.0035\n",
      "Top 20: HR=0.0130, NDCG=0.0049\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0141, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0144, NDCG=0.0053\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0146, NDCG=0.0054\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0042, NDCG=0.0026\n",
      "Top 10: HR=0.0081, NDCG=0.0038\n",
      "Top 20: HR=0.0149, NDCG=0.0055\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0044, NDCG=0.0027\n",
      "Top 10: HR=0.0083, NDCG=0.0039\n",
      "Top 20: HR=0.0154, NDCG=0.0057\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744153002.019822   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0022\n",
      "Top 10: HR=0.0075, NDCG=0.0034\n",
      "Top 20: HR=0.0140, NDCG=0.0050\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0026\n",
      "Top 10: HR=0.0082, NDCG=0.0039\n",
      "Top 20: HR=0.0146, NDCG=0.0055\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0049, NDCG=0.0029\n",
      "Top 10: HR=0.0090, NDCG=0.0042\n",
      "Top 20: HR=0.0159, NDCG=0.0060\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0058, NDCG=0.0034\n",
      "Top 10: HR=0.0102, NDCG=0.0049\n",
      "Top 20: HR=0.0179, NDCG=0.0068\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0071, NDCG=0.0044\n",
      "Top 10: HR=0.0121, NDCG=0.0060\n",
      "Top 20: HR=0.0214, NDCG=0.0084\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744153565.784120   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0036\n",
      "Top 20: HR=0.0140, NDCG=0.0052\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0046, NDCG=0.0027\n",
      "Top 10: HR=0.0082, NDCG=0.0039\n",
      "Top 20: HR=0.0150, NDCG=0.0056\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0053, NDCG=0.0031\n",
      "Top 10: HR=0.0094, NDCG=0.0045\n",
      "Top 20: HR=0.0171, NDCG=0.0064\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0073, NDCG=0.0044\n",
      "Top 10: HR=0.0126, NDCG=0.0061\n",
      "Top 20: HR=0.0224, NDCG=0.0086\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0108, NDCG=0.0067\n",
      "Top 10: HR=0.0184, NDCG=0.0091\n",
      "Top 20: HR=0.0305, NDCG=0.0122\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744154121.820222   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0024\n",
      "Top 10: HR=0.0075, NDCG=0.0035\n",
      "Top 20: HR=0.0137, NDCG=0.0051\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0041, NDCG=0.0025\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0139, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0042, NDCG=0.0026\n",
      "Top 10: HR=0.0077, NDCG=0.0037\n",
      "Top 20: HR=0.0141, NDCG=0.0053\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0045, NDCG=0.0027\n",
      "Top 10: HR=0.0082, NDCG=0.0039\n",
      "Top 20: HR=0.0149, NDCG=0.0056\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0048, NDCG=0.0029\n",
      "Top 10: HR=0.0093, NDCG=0.0044\n",
      "Top 20: HR=0.0166, NDCG=0.0062\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0059, NDCG=0.0035\n",
      "Top 10: HR=0.0113, NDCG=0.0053\n",
      "Top 20: HR=0.0199, NDCG=0.0074\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.3, negative_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744154679.484597   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0037, NDCG=0.0024\n",
      "Top 10: HR=0.0072, NDCG=0.0035\n",
      "Top 20: HR=0.0136, NDCG=0.0050\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0024\n",
      "Top 10: HR=0.0077, NDCG=0.0036\n",
      "Top 20: HR=0.0138, NDCG=0.0052\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0038, NDCG=0.0024\n",
      "Top 10: HR=0.0079, NDCG=0.0037\n",
      "Top 20: HR=0.0137, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0040, NDCG=0.0025\n",
      "Top 10: HR=0.0077, NDCG=0.0037\n",
      "Top 20: HR=0.0142, NDCG=0.0053\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0026\n",
      "Top 10: HR=0.0078, NDCG=0.0037\n",
      "Top 20: HR=0.0143, NDCG=0.0054\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Final evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0044, NDCG=0.0027\n",
      "Top 10: HR=0.0080, NDCG=0.0038\n",
      "Top 20: HR=0.0148, NDCG=0.0055\n",
      "\n",
      "Running experiment with lr=0.005, dropout=0.5, negative_weight=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744155246.054869   11579 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:08:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation:\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0001, NDCG=0.0001\n",
      "Top 10: HR=0.0002, NDCG=0.0001\n",
      "Top 20: HR=0.0005, NDCG=0.0002\n",
      "Epoch 0\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0032, NDCG=0.0019\n",
      "Top 10: HR=0.0071, NDCG=0.0031\n",
      "Top 20: HR=0.0133, NDCG=0.0047\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0039, NDCG=0.0022\n",
      "Top 10: HR=0.0077, NDCG=0.0035\n",
      "Top 20: HR=0.0143, NDCG=0.0051\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Evaluation Metrics:\n",
      "Top 5: HR=0.0043, NDCG=0.0024\n",
      "Top 10: HR=0.0076, NDCG=0.0035\n",
      "Top 20: HR=0.0141, NDCG=0.0051\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 436\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Use fewer epochs for grid search\u001b[39;00m\n\u001b[1;32m    435\u001b[0m args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m--> 436\u001b[0m hr, ndcg \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(((lr, dropout, neg_weight), (hr, ndcg)))\n\u001b[1;32m    438\u001b[0m f_out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal HR@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NDCG@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 387\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(args, data, random_seed)\u001b[0m\n\u001b[1;32m    385\u001b[0m     u_batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39muser_train[start_index:end_index]\n\u001b[1;32m    386\u001b[0m     i_batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mitem_train[start_index:end_index]\n\u001b[0;32m--> 387\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    389\u001b[0m     evaluate(sess, deep, data, args)\n",
      "Cell \u001b[0;32mIn[1], line 316\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(sess, deep, train_op, u_batch, i_batch, args)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_step\u001b[39m(sess, deep, train_op, u_batch, i_batch, args):\n\u001b[1;32m    311\u001b[0m     feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    312\u001b[0m         deep\u001b[38;5;241m.\u001b[39minput_u: u_batch,\n\u001b[1;32m    313\u001b[0m         deep\u001b[38;5;241m.\u001b[39minput_ur: i_batch,\n\u001b[1;32m    314\u001b[0m         deep\u001b[38;5;241m.\u001b[39mdropout_keep_prob: args\u001b[38;5;241m.\u001b[39mdropout\n\u001b[1;32m    315\u001b[0m     }\n\u001b[0;32m--> 316\u001b[0m     _, loss, loss1, reg_loss \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, loss1, reg_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:1220\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1220\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:1400\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1403\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:1407\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1406\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1408\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1409\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:1390\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1388\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ENSFM/lib/python3.9/site-packages/tensorflow/python/client/session.py:1483\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1482\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1483\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.sparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# Dataset Configuration\n",
    "# ==========================\n",
    "# Uncomment the dataset you want to use.\n",
    "# DATASET = 'amazonbook'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "# DATASET = 'ml-1m'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "# DATASET = 'yelp2018'\n",
    "# DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Restaurants'\n",
    "# DATASET = 'frappe'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "# DATASET = 'lastfm'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "\n",
    "# ==========================\n",
    "# Data Loader Definition\n",
    "# ==========================\n",
    "class LoadData(object):\n",
    "    def __init__(self, DATA_ROOT):\n",
    "        self.trainfile = os.path.join(DATA_ROOT, 'train.csv')\n",
    "        self.testfile = os.path.join(DATA_ROOT, 'test.csv')\n",
    "        self.user_field_M, self.item_field_M = self.get_length()\n",
    "        print(\"user_field_M\", self.user_field_M)\n",
    "        print(\"item_field_M\", self.item_field_M)\n",
    "        print(\"Total fields\", self.user_field_M + self.item_field_M)\n",
    "        self.item_bind_M = self.bind_item()   # assigns an ID for each item feature combination\n",
    "        self.user_bind_M = self.bind_user()   # assigns an ID for each user feature combination\n",
    "        print(\"item_bind_M\", len(self.binded_items.values()))\n",
    "        print(\"user_bind_M\", len(self.binded_users.values()))\n",
    "        self.item_map_list = []\n",
    "        for itemid in self.item_map.keys():\n",
    "            self.item_map_list.append([int(feature) for feature in self.item_map[itemid].strip().split('-')])\n",
    "        # Also include mapping for key 0 if it exists\n",
    "        self.item_map_list.append([int(feature) for feature in self.item_map[0].strip().split('-')])\n",
    "        self.user_positive_list = self.get_positive_list(self.trainfile)\n",
    "        self.Train_data, self.Test_data = self.construct_data()\n",
    "        self.user_train, self.item_train = self.get_train_instances()\n",
    "        self.user_test = self.get_test()\n",
    "\n",
    "    def get_length(self):\n",
    "        length_user = 0\n",
    "        length_item = 0\n",
    "        with open(self.trainfile) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                user_features = line.strip().split(',')[0].split('-')\n",
    "                item_features = line.strip().split(',')[1].split('-')\n",
    "                for uf in user_features:\n",
    "                    feature = int(uf)\n",
    "                    if feature > length_user:\n",
    "                        length_user = feature\n",
    "                for itf in item_features:\n",
    "                    feature = int(itf)\n",
    "                    if feature > length_item:\n",
    "                        length_item = feature\n",
    "                line = f.readline()\n",
    "        return length_user + 1, length_item + 1\n",
    "\n",
    "    def bind_item(self):\n",
    "        self.binded_items = {}  # mapping from item feature string to an ID\n",
    "        self.item_map = {}      # mapping from ID to item feature string\n",
    "        self.bind_i(self.trainfile)\n",
    "        self.bind_i(self.testfile)\n",
    "        return len(self.binded_items)\n",
    "\n",
    "    def bind_i(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_items)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                item_features = features[1]\n",
    "                if item_features not in self.binded_items:\n",
    "                    self.binded_items[item_features] = i\n",
    "                    self.item_map[i] = item_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def bind_user(self):\n",
    "        self.binded_users = {}  # mapping from user feature string to an ID\n",
    "        self.user_map = {}      # mapping from ID to user feature string\n",
    "        self.bind_u(self.trainfile)\n",
    "        self.bind_u(self.testfile)\n",
    "        return len(self.binded_users)\n",
    "\n",
    "    def bind_u(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_users)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0]\n",
    "                if user_features not in self.binded_users:\n",
    "                    self.binded_users[user_features] = i\n",
    "                    self.user_map[i] = user_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def get_positive_list(self, file):\n",
    "        self.max_positive_len = 0\n",
    "        user_positive_list = {}\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_id = self.binded_users[features[0]]\n",
    "                item_id = self.binded_items[features[1]]\n",
    "                if user_id in user_positive_list:\n",
    "                    user_positive_list[user_id].append(item_id)\n",
    "                else:\n",
    "                    user_positive_list[user_id] = [item_id]\n",
    "                line = f.readline()\n",
    "        for uid in user_positive_list:\n",
    "            if len(user_positive_list[uid]) > self.max_positive_len:\n",
    "                self.max_positive_len = len(user_positive_list[uid])\n",
    "        return user_positive_list\n",
    "\n",
    "    def get_train_instances(self):\n",
    "        user_train, item_train = [], []\n",
    "        for uid in self.user_positive_list:\n",
    "            u_train = [int(feature) for feature in self.user_map[uid].strip().split('-')]\n",
    "            user_train.append(u_train)\n",
    "            temp = self.user_positive_list[uid][:]\n",
    "            while len(temp) < self.max_positive_len:\n",
    "                temp.append(self.item_bind_M)\n",
    "            item_train.append(temp)\n",
    "        user_train = np.array(user_train)\n",
    "        item_train = np.array(item_train)\n",
    "        return user_train, item_train\n",
    "\n",
    "    def construct_data(self):\n",
    "        X_user, X_item = self.read_data(self.trainfile)\n",
    "        Train_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of training samples:\", len(X_user))\n",
    "        X_user, X_item = self.read_data(self.testfile)\n",
    "        Test_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of test samples:\", len(X_user))\n",
    "        return Train_data, Test_data\n",
    "\n",
    "    def construct_dataset(self, X_user, X_item):\n",
    "        user_id = []\n",
    "        for one in X_user:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            user_id.append(self.binded_users[key])\n",
    "        item_id = []\n",
    "        for one in X_item:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            item_id.append(self.binded_items[key])\n",
    "        count = np.ones(len(X_user))\n",
    "        sparse_matrix = scipy.sparse.csr_matrix((count, (user_id, item_id)),\n",
    "                                                dtype=np.int16,\n",
    "                                                shape=(self.user_bind_M, self.item_bind_M))\n",
    "        return sparse_matrix\n",
    "\n",
    "    def get_test(self):\n",
    "        X_user, _ = self.read_data(self.testfile)\n",
    "        return X_user\n",
    "\n",
    "    def read_data(self, file):\n",
    "        X_user = []\n",
    "        X_item = []\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0].split('-')\n",
    "                X_user.append([int(item) for item in user_features])\n",
    "                item_features = features[1].split('-')\n",
    "                X_item.append([int(item) for item in item_features])\n",
    "                line = f.readline()\n",
    "        return X_user, X_item\n",
    "\n",
    "# ==========================\n",
    "# Model & Training Definitions\n",
    "# ==========================\n",
    "def parse_args():\n",
    "    # For notebooks, we call parse_args with an empty list.\n",
    "    parser = argparse.ArgumentParser(description=\"Run ENSFM with Hyperparameter Search\")\n",
    "    parser.add_argument('--dataset', default='yelp2018',\n",
    "                        help='Dataset name: lastfm, frappe, ml-1m, yelp2018, amazonbook')\n",
    "    parser.add_argument('--batch_size', type=int, default=512, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs for grid search')\n",
    "    parser.add_argument('--verbose', type=int, default=10, help='Evaluation interval (in epochs)')\n",
    "    parser.add_argument('--embed_size', type=int, default=64, help='Embedding size')\n",
    "    parser.add_argument('--lr', type=float, default=0.05, help='Learning rate (overridden in grid search)')\n",
    "    parser.add_argument('--dropout', type=float, default=0.9, help='Dropout keep probability (overridden)')\n",
    "    parser.add_argument('--negative_weight', type=float, default=0.05, help='Negative weight (overridden)')\n",
    "    parser.add_argument('--topK', type=int, nargs='+', default=[5, 10, 20], help='Top K for evaluation')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "def _writeline_and_time(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "    return time.time()\n",
    "\n",
    "class ENSFM:\n",
    "    def __init__(self, item_attribute, user_field_M, item_field_M, embedding_size, max_item_pu, args):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_item_pu = max_item_pu\n",
    "        self.user_field_M = user_field_M\n",
    "        self.item_field_M = item_field_M\n",
    "        self.weight1 = args.negative_weight\n",
    "        self.item_attribute = item_attribute\n",
    "        self.lambda_bilinear = [0.0, 0.0]\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.input_u = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"input_u_feature\")\n",
    "        self.input_ur = tf.compat.v1.placeholder(tf.int32, [None, self.max_item_pu], name=\"input_ur\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    def _create_variables(self):\n",
    "        self.uidW = tf.Variable(tf.random.truncated_normal([self.user_field_M, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"uidW\")\n",
    "        self.iidW = tf.Variable(tf.random.truncated_normal([self.item_field_M+1, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"iidW\")\n",
    "        self.H_i = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_i\")\n",
    "        self.H_s = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_s\")\n",
    "        self.u_bias = tf.Variable(tf.random.truncated_normal([self.user_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"u_bias\")\n",
    "        self.i_bias = tf.Variable(tf.random.truncated_normal([self.item_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"i_bias\")\n",
    "        self.bias = tf.Variable(tf.constant(0.0), name=\"bias\")\n",
    "\n",
    "    def _create_vectors(self):\n",
    "        # User embedding\n",
    "        self.user_feature_emb = tf.nn.embedding_lookup(self.uidW, self.input_u)\n",
    "        self.summed_user_emb = tf.reduce_sum(self.user_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Apply dropout using TF1.x style (keep_prob)\n",
    "        self.H_i_drop = tf.compat.v1.nn.dropout(self.H_i, keep_prob=self.dropout_keep_prob)\n",
    "        self.H_s_drop = tf.compat.v1.nn.dropout(self.H_s, keep_prob=self.dropout_keep_prob)\n",
    "        # Item embedding\n",
    "        self.all_item_feature_emb = tf.nn.embedding_lookup(self.iidW, self.item_attribute)\n",
    "        self.summed_all_item_emb = tf.reduce_sum(self.all_item_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Cross terms\n",
    "        self.user_cross = 0.5 * (tf.square(self.summed_user_emb) - tf.reduce_sum(tf.square(self.user_feature_emb), axis=1))\n",
    "        self.item_cross = 0.5 * (tf.square(self.summed_all_item_emb) - tf.reduce_sum(tf.square(self.all_item_feature_emb), axis=1))\n",
    "        # Compute scores (resulting in [batch, 1, 1])\n",
    "        self.user_cross_score = tf.matmul(tf.expand_dims(self.user_cross, 1), self.H_s_drop)\n",
    "        self.item_cross_score = tf.matmul(tf.expand_dims(self.item_cross, 1), self.H_s_drop)\n",
    "        # Bias terms\n",
    "        self.user_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.u_bias, self.input_u), axis=1)  # [batch, 1]\n",
    "        self.item_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.i_bias, self.item_attribute), axis=1)  # [batch, 1]\n",
    "        # Constant ones\n",
    "        self.I_user = tf.ones([tf.shape(self.input_u)[0], 1])\n",
    "        self.I_item = tf.ones([tf.shape(self.summed_all_item_emb)[0], 1])\n",
    "        # Instead of concatenating rank-3 tensors, squeeze the extra dims so that:\n",
    "        #   - summed_user_emb is [batch, embed_size]\n",
    "        #   - user_cross_score becomes [batch, 1]\n",
    "        #   - user_bias is [batch, 1]\n",
    "        # Then concatenate along axis 1 to get [batch, embed_size+2]\n",
    "        user_cross_score_squeezed = tf.squeeze(self.user_cross_score, axis=1)\n",
    "        user_bias_squeezed = tf.squeeze(tf.expand_dims(self.user_bias, 1), axis=1)\n",
    "        self.p_emb = tf.concat([self.summed_user_emb,\n",
    "                                 user_cross_score_squeezed + user_bias_squeezed + self.bias,\n",
    "                                 self.I_user],\n",
    "                                axis=1)\n",
    "        # Similarly for q_emb\n",
    "        item_cross_score_squeezed = tf.squeeze(self.item_cross_score, axis=1)\n",
    "        self.q_emb = tf.concat([self.summed_all_item_emb,\n",
    "                                 self.I_item,\n",
    "                                 item_cross_score_squeezed + tf.squeeze(tf.expand_dims(self.item_bias, 1), axis=1)],\n",
    "                                axis=1)\n",
    "        # Build H_i_emb by concatenating H_i_drop with two scalars so that its shape becomes [embed_size+2, 1]\n",
    "        self.H_i_emb = tf.concat([self.H_i_drop, [[1.0]], [[1.0]]], axis=0)\n",
    "\n",
    "    def _create_inference(self):\n",
    "        self.pos_item = tf.nn.embedding_lookup(self.q_emb, self.input_ur)\n",
    "        # Assumes that the global data object has attribute item_bind_M\n",
    "        self.pos_num_r = tf.cast(tf.not_equal(self.input_ur, data.item_bind_M), tf.float32)\n",
    "        self.pos_item = tf.einsum('ab,abc->abc', self.pos_num_r, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ac,abc->abc', self.p_emb, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ajk,kl->ajl', self.pos_r, self.H_i_emb)\n",
    "        self.pos_r = tf.reshape(self.pos_r, [-1, self.max_item_pu])\n",
    "\n",
    "    def _pre(self):\n",
    "        dot = tf.einsum('ac,bc->abc', self.p_emb, self.q_emb)\n",
    "        pre = tf.einsum('ajk,kl->aj', dot, self.H_i_emb)\n",
    "        return pre\n",
    "\n",
    "    def _create_loss(self):\n",
    "        self.loss1 = self.weight1 * tf.reduce_sum(\n",
    "            tf.reduce_sum(tf.reduce_sum(tf.einsum('ab,ac->abc', self.q_emb, self.q_emb), axis=0) *\n",
    "                          tf.reduce_sum(tf.einsum('ab,ac->abc', self.p_emb, self.p_emb), axis=0) *\n",
    "                          tf.matmul(self.H_i_emb, self.H_i_emb, transpose_b=True), axis=0), axis=0)\n",
    "        self.loss1 += tf.reduce_sum((1.0 - self.weight1) * tf.square(self.pos_r) - 2.0 * self.pos_r)\n",
    "        self.l2_loss0 = tf.nn.l2_loss(self.uidW)\n",
    "        self.l2_loss1 = tf.nn.l2_loss(self.iidW)\n",
    "        self.loss = self.loss1 + self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "        self.reg_loss = self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._create_variables()\n",
    "        self._create_vectors()\n",
    "        self._create_inference()\n",
    "        self._create_loss()\n",
    "        self.pre = self._pre()\n",
    "\n",
    "def train_step(sess, deep, train_op, u_batch, i_batch, args):\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_ur: i_batch,\n",
    "        deep.dropout_keep_prob: args.dropout\n",
    "    }\n",
    "    _, loss, loss1, reg_loss = sess.run([train_op, deep.loss, deep.loss1, deep.reg_loss], feed_dict)\n",
    "    return loss, loss1, reg_loss\n",
    "\n",
    "def evaluate(sess, deep, data, args):\n",
    "    eva_batch = 128\n",
    "    recall_all = [[] for _ in range(len(args.topK))]\n",
    "    ndcg_all = [[] for _ in range(len(args.topK))]\n",
    "    user_features = data.user_test\n",
    "    num_batches = int(np.ceil(len(user_features) / eva_batch))\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * eva_batch\n",
    "        end_index = min((batch_num + 1) * eva_batch, len(user_features))\n",
    "        u_batch = user_features[start_index:end_index]\n",
    "        batch_users = end_index - start_index\n",
    "        feed_dict = { deep.input_u: u_batch, deep.dropout_keep_prob: 1.0 }\n",
    "        pre = sess.run(deep.pre, feed_dict)\n",
    "        pre = np.array(pre)\n",
    "        pre = np.delete(pre, -1, axis=1)\n",
    "        user_ids = [data.binded_users[\"-\".join(map(str, one))] for one in u_batch]\n",
    "        idx = np.zeros_like(pre, dtype=bool)\n",
    "        idx[data.Train_data[user_ids].nonzero()] = True\n",
    "        pre[idx] = -np.inf\n",
    "        for i, k in enumerate(args.topK):\n",
    "            idx_topk_part = np.argpartition(-pre, k, axis=1)[:, :k]\n",
    "            pre_bin = np.zeros_like(pre, dtype=bool)\n",
    "            pre_bin[np.arange(batch_users)[:, None], idx_topk_part] = True\n",
    "            true_bin = np.zeros_like(pre, dtype=bool)\n",
    "            true_bin[data.Test_data[user_ids].nonzero()] = True\n",
    "            hits = (np.logical_and(true_bin, pre_bin).sum(axis=1)).astype(np.float32)\n",
    "            recall_all[i].append(hits / np.minimum(k, true_bin.sum(axis=1)))\n",
    "            topk_part = pre[np.arange(batch_users)[:, None], idx_topk_part]\n",
    "            idx_part = np.argsort(-topk_part, axis=1)\n",
    "            idx_topk = idx_topk_part[np.arange(batch_users)[:, None], idx_part]\n",
    "            tp = np.log(2) / np.log(np.arange(2, k + 2))\n",
    "            test_batch = data.Test_data[user_ids]\n",
    "            DCG = (test_batch[np.arange(batch_users)[:, None], idx_topk].toarray() * tp).sum(axis=1)\n",
    "            IDCG = np.array([(tp[:min(n, k)]).sum() for n in test_batch.getnnz(axis=1)])\n",
    "            ndcg_all[i].append(DCG / IDCG)\n",
    "    recalls = [np.mean(np.hstack(r)) for r in recall_all]\n",
    "    ndcgs = [np.mean(np.hstack(n)) for n in ndcg_all]\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for i, k in enumerate(args.topK):\n",
    "        print(f\"Top {k}: HR={recalls[i]:.4f}, NDCG={ndcgs[i]:.4f}\")\n",
    "    return recalls, ndcgs\n",
    "\n",
    "def run_experiment(args, data, random_seed=2019):\n",
    "    with tf.Graph().as_default():\n",
    "        tf.compat.v1.set_random_seed(random_seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto()\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = ENSFM(data.item_map_list, data.user_field_M, data.item_field_M,\n",
    "                         args.embed_size, data.max_positive_len, args)\n",
    "            deep._build_graph()\n",
    "            train_op = tf.compat.v1.train.AdagradOptimizer(\n",
    "                learning_rate=args.lr, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            print(\"Initial evaluation:\")\n",
    "            evaluate(sess, deep, data, args)\n",
    "            for epoch in range(args.epochs):\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                shuffle_idx = np.random.permutation(len(data.user_train))\n",
    "                data.user_train = data.user_train[shuffle_idx]\n",
    "                data.item_train = data.item_train[shuffle_idx]\n",
    "                num_batches = int(np.ceil(len(data.user_train) / args.batch_size))\n",
    "                for batch_num in range(num_batches):\n",
    "                    start_index = batch_num * args.batch_size\n",
    "                    end_index = min((batch_num + 1) * args.batch_size, len(data.user_train))\n",
    "                    u_batch = data.user_train[start_index:end_index]\n",
    "                    i_batch = data.item_train[start_index:end_index]\n",
    "                    train_step(sess, deep, train_op, u_batch, i_batch, args)\n",
    "                if epoch % args.verbose == 0:\n",
    "                    evaluate(sess, deep, data, args)\n",
    "            print(\"Final evaluation:\")\n",
    "            recalls, ndcgs = evaluate(sess, deep, data, args)\n",
    "    hr_index = args.topK.index(10) if 10 in args.topK else 0\n",
    "    return recalls[hr_index], ndcgs[hr_index]\n",
    "\n",
    "# ==========================\n",
    "# Main Hyperparameter Search\n",
    "# ==========================\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    # Optionally update DATA_ROOT based on args.dataset\n",
    "    if args.dataset == 'lastfm':\n",
    "        print(\"Loading LastFM data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "    elif args.dataset == 'frappe':\n",
    "        print(\"Loading Frappe data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "    elif args.dataset == 'ml-1m':\n",
    "        print(\"Loading ML-1M data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "    elif args.dataset == 'yelp2018':\n",
    "        print(\"Loading Yelp2018 data\")\n",
    "        DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Restaurants'\n",
    "    elif args.dataset == 'amazonbook':\n",
    "        print(\"Loading Amazon Book data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "\n",
    "    # Load data\n",
    "    data = LoadData(DATA_ROOT)\n",
    "\n",
    "    # Define hyperparameter grid.\n",
    "    lr_values = [0.005, 0.01, 0.02, 0.05]\n",
    "    dropout_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "    neg_weight_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "    results = []\n",
    "    output_file = os.path.join(DATA_ROOT, 'ENSFM_hyperparam_results.txt')\n",
    "    with open(output_file, 'w') as f_out:\n",
    "        for lr, dropout, neg_weight in itertools.product(lr_values, dropout_values, neg_weight_values):\n",
    "            print(f\"\\nRunning experiment with lr={lr}, dropout={dropout}, negative_weight={neg_weight}\")\n",
    "            f_out.write(f\"Parameters: lr={lr}, dropout={dropout}, negative_weight={neg_weight}\\n\")\n",
    "            args.lr = lr\n",
    "            args.dropout = dropout\n",
    "            args.negative_weight = neg_weight\n",
    "            # Use fewer epochs for grid search\n",
    "            args.epochs = 50\n",
    "            hr, ndcg = run_experiment(args, data, random_seed=2019)\n",
    "            results.append(((lr, dropout, neg_weight), (hr, ndcg)))\n",
    "            f_out.write(f\"Final HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\\n\\n\")\n",
    "            f_out.flush()\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x[1][0], reverse=True)\n",
    "    print(\"\\nSorted hyperparameter search results (by HR@10):\")\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        f_out.write(\"Sorted hyperparameter search results (by HR@10):\\n\")\n",
    "        for combo, metrics in sorted_results:\n",
    "            line = f\"Parameters (lr, dropout, negative_weight): {combo} => HR@10: {metrics[0]:.4f}, NDCG@10: {metrics[1]:.4f}\\n\"\n",
    "            print(line.strip())\n",
    "            f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546a357",
   "metadata": {},
   "source": [
    "## Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02267b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.sparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# Dataset Configuration\n",
    "# ==========================\n",
    "# Uncomment the dataset you want to use.\n",
    "# DATASET = 'amazonbook'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "# DATASET = 'ml-1m'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "# DATASET = 'yelp2018'\n",
    "# DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Restaurants'\n",
    "# DATASET = 'frappe'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "# DATASET = 'lastfm'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "\n",
    "# ==========================\n",
    "# Data Loader Definition\n",
    "# ==========================\n",
    "class LoadData(object):\n",
    "    def __init__(self, DATA_ROOT):\n",
    "        self.trainfile = os.path.join(DATA_ROOT, 'train.csv')\n",
    "        self.testfile = os.path.join(DATA_ROOT, 'test.csv')\n",
    "        self.user_field_M, self.item_field_M = self.get_length()\n",
    "        print(\"user_field_M\", self.user_field_M)\n",
    "        print(\"item_field_M\", self.item_field_M)\n",
    "        print(\"Total fields\", self.user_field_M + self.item_field_M)\n",
    "        self.item_bind_M = self.bind_item()   # assigns an ID for each item feature combination\n",
    "        self.user_bind_M = self.bind_user()   # assigns an ID for each user feature combination\n",
    "        print(\"item_bind_M\", len(self.binded_items.values()))\n",
    "        print(\"user_bind_M\", len(self.binded_users.values()))\n",
    "        self.item_map_list = []\n",
    "        for itemid in self.item_map.keys():\n",
    "            self.item_map_list.append([int(feature) for feature in self.item_map[itemid].strip().split('-')])\n",
    "        # Also include mapping for key 0 if it exists\n",
    "        self.item_map_list.append([int(feature) for feature in self.item_map[0].strip().split('-')])\n",
    "        self.user_positive_list = self.get_positive_list(self.trainfile)\n",
    "        self.Train_data, self.Test_data = self.construct_data()\n",
    "        self.user_train, self.item_train = self.get_train_instances()\n",
    "        self.user_test = self.get_test()\n",
    "\n",
    "    def get_length(self):\n",
    "        length_user = 0\n",
    "        length_item = 0\n",
    "        with open(self.trainfile) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                user_features = line.strip().split(',')[0].split('-')\n",
    "                item_features = line.strip().split(',')[1].split('-')\n",
    "                for uf in user_features:\n",
    "                    feature = int(uf)\n",
    "                    if feature > length_user:\n",
    "                        length_user = feature\n",
    "                for itf in item_features:\n",
    "                    feature = int(itf)\n",
    "                    if feature > length_item:\n",
    "                        length_item = feature\n",
    "                line = f.readline()\n",
    "        return length_user + 1, length_item + 1\n",
    "\n",
    "    def bind_item(self):\n",
    "        self.binded_items = {}  # mapping from item feature string to an ID\n",
    "        self.item_map = {}      # mapping from ID to item feature string\n",
    "        self.bind_i(self.trainfile)\n",
    "        self.bind_i(self.testfile)\n",
    "        return len(self.binded_items)\n",
    "\n",
    "    def bind_i(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_items)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                item_features = features[1]\n",
    "                if item_features not in self.binded_items:\n",
    "                    self.binded_items[item_features] = i\n",
    "                    self.item_map[i] = item_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def bind_user(self):\n",
    "        self.binded_users = {}  # mapping from user feature string to an ID\n",
    "        self.user_map = {}      # mapping from ID to user feature string\n",
    "        self.bind_u(self.trainfile)\n",
    "        self.bind_u(self.testfile)\n",
    "        return len(self.binded_users)\n",
    "\n",
    "    def bind_u(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_users)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0]\n",
    "                if user_features not in self.binded_users:\n",
    "                    self.binded_users[user_features] = i\n",
    "                    self.user_map[i] = user_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def get_positive_list(self, file):\n",
    "        self.max_positive_len = 0\n",
    "        user_positive_list = {}\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_id = self.binded_users[features[0]]\n",
    "                item_id = self.binded_items[features[1]]\n",
    "                if user_id in user_positive_list:\n",
    "                    user_positive_list[user_id].append(item_id)\n",
    "                else:\n",
    "                    user_positive_list[user_id] = [item_id]\n",
    "                line = f.readline()\n",
    "        for uid in user_positive_list:\n",
    "            if len(user_positive_list[uid]) > self.max_positive_len:\n",
    "                self.max_positive_len = len(user_positive_list[uid])\n",
    "        return user_positive_list\n",
    "\n",
    "    def get_train_instances(self):\n",
    "        user_train, item_train = [], []\n",
    "        for uid in self.user_positive_list:\n",
    "            u_train = [int(feature) for feature in self.user_map[uid].strip().split('-')]\n",
    "            user_train.append(u_train)\n",
    "            temp = self.user_positive_list[uid][:]\n",
    "            while len(temp) < self.max_positive_len:\n",
    "                temp.append(self.item_bind_M)\n",
    "            item_train.append(temp)\n",
    "        user_train = np.array(user_train)\n",
    "        item_train = np.array(item_train)\n",
    "        return user_train, item_train\n",
    "\n",
    "    def construct_data(self):\n",
    "        X_user, X_item = self.read_data(self.trainfile)\n",
    "        Train_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of training samples:\", len(X_user))\n",
    "        X_user, X_item = self.read_data(self.testfile)\n",
    "        Test_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of test samples:\", len(X_user))\n",
    "        return Train_data, Test_data\n",
    "\n",
    "    def construct_dataset(self, X_user, X_item):\n",
    "        user_id = []\n",
    "        for one in X_user:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            user_id.append(self.binded_users[key])\n",
    "        item_id = []\n",
    "        for one in X_item:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            item_id.append(self.binded_items[key])\n",
    "        count = np.ones(len(X_user))\n",
    "        sparse_matrix = scipy.sparse.csr_matrix((count, (user_id, item_id)),\n",
    "                                                dtype=np.int16,\n",
    "                                                shape=(self.user_bind_M, self.item_bind_M))\n",
    "        return sparse_matrix\n",
    "\n",
    "    def get_test(self):\n",
    "        X_user, _ = self.read_data(self.testfile)\n",
    "        return X_user\n",
    "\n",
    "    def read_data(self, file):\n",
    "        X_user = []\n",
    "        X_item = []\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0].split('-')\n",
    "                X_user.append([int(item) for item in user_features])\n",
    "                item_features = features[1].split('-')\n",
    "                X_item.append([int(item) for item in item_features])\n",
    "                line = f.readline()\n",
    "        return X_user, X_item\n",
    "\n",
    "# ==========================\n",
    "# Model & Training Definitions\n",
    "# ==========================\n",
    "def parse_args():\n",
    "    # For notebooks, we call parse_args with an empty list.\n",
    "    parser = argparse.ArgumentParser(description=\"Run ENSFM with Hyperparameter Search\")\n",
    "    parser.add_argument('--dataset', default='yelp2018',\n",
    "                        help='Dataset name: lastfm, frappe, ml-1m, yelp2018, amazonbook')\n",
    "    parser.add_argument('--batch_size', type=int, default=512, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs for grid search')\n",
    "    parser.add_argument('--verbose', type=int, default=10, help='Evaluation interval (in epochs)')\n",
    "    parser.add_argument('--embed_size', type=int, default=64, help='Embedding size')\n",
    "    parser.add_argument('--lr', type=float, default=0.05, help='Learning rate (overridden in grid search)')\n",
    "    parser.add_argument('--dropout', type=float, default=0.9, help='Dropout keep probability (overridden)')\n",
    "    parser.add_argument('--negative_weight', type=float, default=0.05, help='Negative weight (overridden)')\n",
    "    parser.add_argument('--topK', type=int, nargs='+', default=[5, 10, 20], help='Top K for evaluation')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "def _writeline_and_time(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "    return time.time()\n",
    "\n",
    "class ENSFM:\n",
    "    def __init__(self, item_attribute, user_field_M, item_field_M, embedding_size, max_item_pu, args):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_item_pu = max_item_pu\n",
    "        self.user_field_M = user_field_M\n",
    "        self.item_field_M = item_field_M\n",
    "        self.weight1 = args.negative_weight\n",
    "        self.item_attribute = item_attribute\n",
    "        self.lambda_bilinear = [0.0, 0.0]\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.input_u = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"input_u_feature\")\n",
    "        self.input_ur = tf.compat.v1.placeholder(tf.int32, [None, self.max_item_pu], name=\"input_ur\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    def _create_variables(self):\n",
    "        self.uidW = tf.Variable(tf.random.truncated_normal([self.user_field_M, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"uidW\")\n",
    "        self.iidW = tf.Variable(tf.random.truncated_normal([self.item_field_M+1, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"iidW\")\n",
    "        self.H_i = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_i\")\n",
    "        self.H_s = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_s\")\n",
    "        self.u_bias = tf.Variable(tf.random.truncated_normal([self.user_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"u_bias\")\n",
    "        self.i_bias = tf.Variable(tf.random.truncated_normal([self.item_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"i_bias\")\n",
    "        self.bias = tf.Variable(tf.constant(0.0), name=\"bias\")\n",
    "\n",
    "    def _create_vectors(self):\n",
    "        # User embedding\n",
    "        self.user_feature_emb = tf.nn.embedding_lookup(self.uidW, self.input_u)\n",
    "        self.summed_user_emb = tf.reduce_sum(self.user_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Apply dropout using TF1.x style (keep_prob)\n",
    "        self.H_i_drop = tf.compat.v1.nn.dropout(self.H_i, keep_prob=self.dropout_keep_prob)\n",
    "        self.H_s_drop = tf.compat.v1.nn.dropout(self.H_s, keep_prob=self.dropout_keep_prob)\n",
    "        # Item embedding\n",
    "        self.all_item_feature_emb = tf.nn.embedding_lookup(self.iidW, self.item_attribute)\n",
    "        self.summed_all_item_emb = tf.reduce_sum(self.all_item_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Cross terms\n",
    "        self.user_cross = 0.5 * (tf.square(self.summed_user_emb) - tf.reduce_sum(tf.square(self.user_feature_emb), axis=1))\n",
    "        self.item_cross = 0.5 * (tf.square(self.summed_all_item_emb) - tf.reduce_sum(tf.square(self.all_item_feature_emb), axis=1))\n",
    "        # Compute scores (resulting in [batch, 1, 1])\n",
    "        self.user_cross_score = tf.matmul(tf.expand_dims(self.user_cross, 1), self.H_s_drop)\n",
    "        self.item_cross_score = tf.matmul(tf.expand_dims(self.item_cross, 1), self.H_s_drop)\n",
    "        # Bias terms\n",
    "        self.user_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.u_bias, self.input_u), axis=1)  # [batch, 1]\n",
    "        self.item_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.i_bias, self.item_attribute), axis=1)  # [batch, 1]\n",
    "        # Constant ones\n",
    "        self.I_user = tf.ones([tf.shape(self.input_u)[0], 1])\n",
    "        self.I_item = tf.ones([tf.shape(self.summed_all_item_emb)[0], 1])\n",
    "        # Instead of concatenating rank-3 tensors, squeeze the extra dims so that:\n",
    "        #   - summed_user_emb is [batch, embed_size]\n",
    "        #   - user_cross_score becomes [batch, 1]\n",
    "        #   - user_bias is [batch, 1]\n",
    "        # Then concatenate along axis 1 to get [batch, embed_size+2]\n",
    "        user_cross_score_squeezed = tf.squeeze(self.user_cross_score, axis=1)\n",
    "        user_bias_squeezed = tf.squeeze(tf.expand_dims(self.user_bias, 1), axis=1)\n",
    "        self.p_emb = tf.concat([self.summed_user_emb,\n",
    "                                 user_cross_score_squeezed + user_bias_squeezed + self.bias,\n",
    "                                 self.I_user],\n",
    "                                axis=1)\n",
    "        # Similarly for q_emb\n",
    "        item_cross_score_squeezed = tf.squeeze(self.item_cross_score, axis=1)\n",
    "        self.q_emb = tf.concat([self.summed_all_item_emb,\n",
    "                                 self.I_item,\n",
    "                                 item_cross_score_squeezed + tf.squeeze(tf.expand_dims(self.item_bias, 1), axis=1)],\n",
    "                                axis=1)\n",
    "        # Build H_i_emb by concatenating H_i_drop with two scalars so that its shape becomes [embed_size+2, 1]\n",
    "        self.H_i_emb = tf.concat([self.H_i_drop, [[1.0]], [[1.0]]], axis=0)\n",
    "\n",
    "    def _create_inference(self):\n",
    "        self.pos_item = tf.nn.embedding_lookup(self.q_emb, self.input_ur)\n",
    "        # Assumes that the global data object has attribute item_bind_M\n",
    "        self.pos_num_r = tf.cast(tf.not_equal(self.input_ur, data.item_bind_M), tf.float32)\n",
    "        self.pos_item = tf.einsum('ab,abc->abc', self.pos_num_r, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ac,abc->abc', self.p_emb, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ajk,kl->ajl', self.pos_r, self.H_i_emb)\n",
    "        self.pos_r = tf.reshape(self.pos_r, [-1, self.max_item_pu])\n",
    "\n",
    "    def _pre(self):\n",
    "        dot = tf.einsum('ac,bc->abc', self.p_emb, self.q_emb)\n",
    "        pre = tf.einsum('ajk,kl->aj', dot, self.H_i_emb)\n",
    "        return pre\n",
    "\n",
    "    def _create_loss(self):\n",
    "        self.loss1 = self.weight1 * tf.reduce_sum(\n",
    "            tf.reduce_sum(tf.reduce_sum(tf.einsum('ab,ac->abc', self.q_emb, self.q_emb), axis=0) *\n",
    "                          tf.reduce_sum(tf.einsum('ab,ac->abc', self.p_emb, self.p_emb), axis=0) *\n",
    "                          tf.matmul(self.H_i_emb, self.H_i_emb, transpose_b=True), axis=0), axis=0)\n",
    "        self.loss1 += tf.reduce_sum((1.0 - self.weight1) * tf.square(self.pos_r) - 2.0 * self.pos_r)\n",
    "        self.l2_loss0 = tf.nn.l2_loss(self.uidW)\n",
    "        self.l2_loss1 = tf.nn.l2_loss(self.iidW)\n",
    "        self.loss = self.loss1 + self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "        self.reg_loss = self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._create_variables()\n",
    "        self._create_vectors()\n",
    "        self._create_inference()\n",
    "        self._create_loss()\n",
    "        self.pre = self._pre()\n",
    "\n",
    "def train_step(sess, deep, train_op, u_batch, i_batch, args):\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_ur: i_batch,\n",
    "        deep.dropout_keep_prob: args.dropout\n",
    "    }\n",
    "    _, loss, loss1, reg_loss = sess.run([train_op, deep.loss, deep.loss1, deep.reg_loss], feed_dict)\n",
    "    return loss, loss1, reg_loss\n",
    "\n",
    "def evaluate(sess, deep, data, args):\n",
    "    eva_batch = 128\n",
    "    recall_all = [[] for _ in range(len(args.topK))]\n",
    "    ndcg_all = [[] for _ in range(len(args.topK))]\n",
    "    user_features = data.user_test\n",
    "    num_batches = int(np.ceil(len(user_features) / eva_batch))\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * eva_batch\n",
    "        end_index = min((batch_num + 1) * eva_batch, len(user_features))\n",
    "        u_batch = user_features[start_index:end_index]\n",
    "        batch_users = end_index - start_index\n",
    "        feed_dict = { deep.input_u: u_batch, deep.dropout_keep_prob: 1.0 }\n",
    "        pre = sess.run(deep.pre, feed_dict)\n",
    "        pre = np.array(pre)\n",
    "        pre = np.delete(pre, -1, axis=1)\n",
    "        user_ids = [data.binded_users[\"-\".join(map(str, one))] for one in u_batch]\n",
    "        idx = np.zeros_like(pre, dtype=bool)\n",
    "        idx[data.Train_data[user_ids].nonzero()] = True\n",
    "        pre[idx] = -np.inf\n",
    "        for i, k in enumerate(args.topK):\n",
    "            idx_topk_part = np.argpartition(-pre, k, axis=1)[:, :k]\n",
    "            pre_bin = np.zeros_like(pre, dtype=bool)\n",
    "            pre_bin[np.arange(batch_users)[:, None], idx_topk_part] = True\n",
    "            true_bin = np.zeros_like(pre, dtype=bool)\n",
    "            true_bin[data.Test_data[user_ids].nonzero()] = True\n",
    "            hits = (np.logical_and(true_bin, pre_bin).sum(axis=1)).astype(np.float32)\n",
    "            recall_all[i].append(hits / np.minimum(k, true_bin.sum(axis=1)))\n",
    "            topk_part = pre[np.arange(batch_users)[:, None], idx_topk_part]\n",
    "            idx_part = np.argsort(-topk_part, axis=1)\n",
    "            idx_topk = idx_topk_part[np.arange(batch_users)[:, None], idx_part]\n",
    "            tp = np.log(2) / np.log(np.arange(2, k + 2))\n",
    "            test_batch = data.Test_data[user_ids]\n",
    "            DCG = (test_batch[np.arange(batch_users)[:, None], idx_topk].toarray() * tp).sum(axis=1)\n",
    "            IDCG = np.array([(tp[:min(n, k)]).sum() for n in test_batch.getnnz(axis=1)])\n",
    "            ndcg_all[i].append(DCG / IDCG)\n",
    "    recalls = [np.mean(np.hstack(r)) for r in recall_all]\n",
    "    ndcgs = [np.mean(np.hstack(n)) for n in ndcg_all]\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for i, k in enumerate(args.topK):\n",
    "        print(f\"Top {k}: HR={recalls[i]:.4f}, NDCG={ndcgs[i]:.4f}\")\n",
    "    return recalls, ndcgs\n",
    "\n",
    "def run_experiment(args, data, random_seed=2019):\n",
    "    with tf.Graph().as_default():\n",
    "        tf.compat.v1.set_random_seed(random_seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto()\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = ENSFM(data.item_map_list, data.user_field_M, data.item_field_M,\n",
    "                         args.embed_size, data.max_positive_len, args)\n",
    "            deep._build_graph()\n",
    "            train_op = tf.compat.v1.train.AdagradOptimizer(\n",
    "                learning_rate=args.lr, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            print(\"Initial evaluation:\")\n",
    "            evaluate(sess, deep, data, args)\n",
    "            for epoch in range(args.epochs):\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                shuffle_idx = np.random.permutation(len(data.user_train))\n",
    "                data.user_train = data.user_train[shuffle_idx]\n",
    "                data.item_train = data.item_train[shuffle_idx]\n",
    "                num_batches = int(np.ceil(len(data.user_train) / args.batch_size))\n",
    "                for batch_num in range(num_batches):\n",
    "                    start_index = batch_num * args.batch_size\n",
    "                    end_index = min((batch_num + 1) * args.batch_size, len(data.user_train))\n",
    "                    u_batch = data.user_train[start_index:end_index]\n",
    "                    i_batch = data.item_train[start_index:end_index]\n",
    "                    train_step(sess, deep, train_op, u_batch, i_batch, args)\n",
    "                if epoch % args.verbose == 0:\n",
    "                    evaluate(sess, deep, data, args)\n",
    "            print(\"Final evaluation:\")\n",
    "            recalls, ndcgs = evaluate(sess, deep, data, args)\n",
    "    hr_index = args.topK.index(10) if 10 in args.topK else 0\n",
    "    return recalls[hr_index], ndcgs[hr_index]\n",
    "\n",
    "# ==========================\n",
    "# Main Hyperparameter Search\n",
    "# ==========================\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    # Optionally update DATA_ROOT based on args.dataset\n",
    "    if args.dataset == 'lastfm':\n",
    "        print(\"Loading LastFM data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "    elif args.dataset == 'frappe':\n",
    "        print(\"Loading Frappe data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "    elif args.dataset == 'ml-1m':\n",
    "        print(\"Loading ML-1M data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "    elif args.dataset == 'yelp2018':\n",
    "        print(\"Loading Yelp2018 data\")\n",
    "        DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Food'\n",
    "    elif args.dataset == 'amazonbook':\n",
    "        print(\"Loading Amazon Book data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "\n",
    "    # Load data\n",
    "    data = LoadData(DATA_ROOT)\n",
    "\n",
    "    # Define hyperparameter grid.\n",
    "    lr_values = [0.005, 0.01, 0.02, 0.05]\n",
    "    dropout_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "    neg_weight_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "    results = []\n",
    "    output_file = os.path.join(DATA_ROOT, 'ENSFM_hyperparam_results.txt')\n",
    "    with open(output_file, 'w') as f_out:\n",
    "        for lr, dropout, neg_weight in itertools.product(lr_values, dropout_values, neg_weight_values):\n",
    "            print(f\"\\nRunning experiment with lr={lr}, dropout={dropout}, negative_weight={neg_weight}\")\n",
    "            f_out.write(f\"Parameters: lr={lr}, dropout={dropout}, negative_weight={neg_weight}\\n\")\n",
    "            args.lr = lr\n",
    "            args.dropout = dropout\n",
    "            args.negative_weight = neg_weight\n",
    "            # Use fewer epochs for grid search\n",
    "            args.epochs = 50\n",
    "            hr, ndcg = run_experiment(args, data, random_seed=2019)\n",
    "            results.append(((lr, dropout, neg_weight), (hr, ndcg)))\n",
    "            f_out.write(f\"Final HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\\n\\n\")\n",
    "            f_out.flush()\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x[1][0], reverse=True)\n",
    "    print(\"\\nSorted hyperparameter search results (by HR@10):\")\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        f_out.write(\"Sorted hyperparameter search results (by HR@10):\\n\")\n",
    "        for combo, metrics in sorted_results:\n",
    "            line = f\"Parameters (lr, dropout, negative_weight): {combo} => HR@10: {metrics[0]:.4f}, NDCG@10: {metrics[1]:.4f}\\n\"\n",
    "            print(line.strip())\n",
    "            f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe44e69",
   "metadata": {},
   "source": [
    "## Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.sparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# Dataset Configuration\n",
    "# ==========================\n",
    "# Uncomment the dataset you want to use.\n",
    "# DATASET = 'amazonbook'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "# DATASET = 'ml-1m'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "# DATASET = 'yelp2018'\n",
    "# DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Restaurants'\n",
    "# DATASET = 'frappe'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "# DATASET = 'lastfm'\n",
    "# DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "\n",
    "# ==========================\n",
    "# Data Loader Definition\n",
    "# ==========================\n",
    "class LoadData(object):\n",
    "    def __init__(self, DATA_ROOT):\n",
    "        self.trainfile = os.path.join(DATA_ROOT, 'train.csv')\n",
    "        self.testfile = os.path.join(DATA_ROOT, 'test.csv')\n",
    "        self.user_field_M, self.item_field_M = self.get_length()\n",
    "        print(\"user_field_M\", self.user_field_M)\n",
    "        print(\"item_field_M\", self.item_field_M)\n",
    "        print(\"Total fields\", self.user_field_M + self.item_field_M)\n",
    "        self.item_bind_M = self.bind_item()   # assigns an ID for each item feature combination\n",
    "        self.user_bind_M = self.bind_user()   # assigns an ID for each user feature combination\n",
    "        print(\"item_bind_M\", len(self.binded_items.values()))\n",
    "        print(\"user_bind_M\", len(self.binded_users.values()))\n",
    "        self.item_map_list = []\n",
    "        for itemid in self.item_map.keys():\n",
    "            self.item_map_list.append([int(feature) for feature in self.item_map[itemid].strip().split('-')])\n",
    "        # Also include mapping for key 0 if it exists\n",
    "        self.item_map_list.append([int(feature) for feature in self.item_map[0].strip().split('-')])\n",
    "        self.user_positive_list = self.get_positive_list(self.trainfile)\n",
    "        self.Train_data, self.Test_data = self.construct_data()\n",
    "        self.user_train, self.item_train = self.get_train_instances()\n",
    "        self.user_test = self.get_test()\n",
    "\n",
    "    def get_length(self):\n",
    "        length_user = 0\n",
    "        length_item = 0\n",
    "        with open(self.trainfile) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                user_features = line.strip().split(',')[0].split('-')\n",
    "                item_features = line.strip().split(',')[1].split('-')\n",
    "                for uf in user_features:\n",
    "                    feature = int(uf)\n",
    "                    if feature > length_user:\n",
    "                        length_user = feature\n",
    "                for itf in item_features:\n",
    "                    feature = int(itf)\n",
    "                    if feature > length_item:\n",
    "                        length_item = feature\n",
    "                line = f.readline()\n",
    "        return length_user + 1, length_item + 1\n",
    "\n",
    "    def bind_item(self):\n",
    "        self.binded_items = {}  # mapping from item feature string to an ID\n",
    "        self.item_map = {}      # mapping from ID to item feature string\n",
    "        self.bind_i(self.trainfile)\n",
    "        self.bind_i(self.testfile)\n",
    "        return len(self.binded_items)\n",
    "\n",
    "    def bind_i(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_items)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                item_features = features[1]\n",
    "                if item_features not in self.binded_items:\n",
    "                    self.binded_items[item_features] = i\n",
    "                    self.item_map[i] = item_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def bind_user(self):\n",
    "        self.binded_users = {}  # mapping from user feature string to an ID\n",
    "        self.user_map = {}      # mapping from ID to user feature string\n",
    "        self.bind_u(self.trainfile)\n",
    "        self.bind_u(self.testfile)\n",
    "        return len(self.binded_users)\n",
    "\n",
    "    def bind_u(self, file):\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            i = len(self.binded_users)\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0]\n",
    "                if user_features not in self.binded_users:\n",
    "                    self.binded_users[user_features] = i\n",
    "                    self.user_map[i] = user_features\n",
    "                    i += 1\n",
    "                line = f.readline()\n",
    "\n",
    "    def get_positive_list(self, file):\n",
    "        self.max_positive_len = 0\n",
    "        user_positive_list = {}\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_id = self.binded_users[features[0]]\n",
    "                item_id = self.binded_items[features[1]]\n",
    "                if user_id in user_positive_list:\n",
    "                    user_positive_list[user_id].append(item_id)\n",
    "                else:\n",
    "                    user_positive_list[user_id] = [item_id]\n",
    "                line = f.readline()\n",
    "        for uid in user_positive_list:\n",
    "            if len(user_positive_list[uid]) > self.max_positive_len:\n",
    "                self.max_positive_len = len(user_positive_list[uid])\n",
    "        return user_positive_list\n",
    "\n",
    "    def get_train_instances(self):\n",
    "        user_train, item_train = [], []\n",
    "        for uid in self.user_positive_list:\n",
    "            u_train = [int(feature) for feature in self.user_map[uid].strip().split('-')]\n",
    "            user_train.append(u_train)\n",
    "            temp = self.user_positive_list[uid][:]\n",
    "            while len(temp) < self.max_positive_len:\n",
    "                temp.append(self.item_bind_M)\n",
    "            item_train.append(temp)\n",
    "        user_train = np.array(user_train)\n",
    "        item_train = np.array(item_train)\n",
    "        return user_train, item_train\n",
    "\n",
    "    def construct_data(self):\n",
    "        X_user, X_item = self.read_data(self.trainfile)\n",
    "        Train_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of training samples:\", len(X_user))\n",
    "        X_user, X_item = self.read_data(self.testfile)\n",
    "        Test_data = self.construct_dataset(X_user, X_item)\n",
    "        print(\"# of test samples:\", len(X_user))\n",
    "        return Train_data, Test_data\n",
    "\n",
    "    def construct_dataset(self, X_user, X_item):\n",
    "        user_id = []\n",
    "        for one in X_user:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            user_id.append(self.binded_users[key])\n",
    "        item_id = []\n",
    "        for one in X_item:\n",
    "            key = \"-\".join([str(item) for item in one])\n",
    "            item_id.append(self.binded_items[key])\n",
    "        count = np.ones(len(X_user))\n",
    "        sparse_matrix = scipy.sparse.csr_matrix((count, (user_id, item_id)),\n",
    "                                                dtype=np.int16,\n",
    "                                                shape=(self.user_bind_M, self.item_bind_M))\n",
    "        return sparse_matrix\n",
    "\n",
    "    def get_test(self):\n",
    "        X_user, _ = self.read_data(self.testfile)\n",
    "        return X_user\n",
    "\n",
    "    def read_data(self, file):\n",
    "        X_user = []\n",
    "        X_item = []\n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                features = line.strip().split(',')\n",
    "                user_features = features[0].split('-')\n",
    "                X_user.append([int(item) for item in user_features])\n",
    "                item_features = features[1].split('-')\n",
    "                X_item.append([int(item) for item in item_features])\n",
    "                line = f.readline()\n",
    "        return X_user, X_item\n",
    "\n",
    "# ==========================\n",
    "# Model & Training Definitions\n",
    "# ==========================\n",
    "def parse_args():\n",
    "    # For notebooks, we call parse_args with an empty list.\n",
    "    parser = argparse.ArgumentParser(description=\"Run ENSFM with Hyperparameter Search\")\n",
    "    parser.add_argument('--dataset', default='yelp2018',\n",
    "                        help='Dataset name: lastfm, frappe, ml-1m, yelp2018, amazonbook')\n",
    "    parser.add_argument('--batch_size', type=int, default=512, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs for grid search')\n",
    "    parser.add_argument('--verbose', type=int, default=10, help='Evaluation interval (in epochs)')\n",
    "    parser.add_argument('--embed_size', type=int, default=64, help='Embedding size')\n",
    "    parser.add_argument('--lr', type=float, default=0.05, help='Learning rate (overridden in grid search)')\n",
    "    parser.add_argument('--dropout', type=float, default=0.9, help='Dropout keep probability (overridden)')\n",
    "    parser.add_argument('--negative_weight', type=float, default=0.05, help='Negative weight (overridden)')\n",
    "    parser.add_argument('--topK', type=int, nargs='+', default=[5, 10, 20], help='Top K for evaluation')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "def _writeline_and_time(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "    return time.time()\n",
    "\n",
    "class ENSFM:\n",
    "    def __init__(self, item_attribute, user_field_M, item_field_M, embedding_size, max_item_pu, args):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_item_pu = max_item_pu\n",
    "        self.user_field_M = user_field_M\n",
    "        self.item_field_M = item_field_M\n",
    "        self.weight1 = args.negative_weight\n",
    "        self.item_attribute = item_attribute\n",
    "        self.lambda_bilinear = [0.0, 0.0]\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.input_u = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"input_u_feature\")\n",
    "        self.input_ur = tf.compat.v1.placeholder(tf.int32, [None, self.max_item_pu], name=\"input_ur\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    def _create_variables(self):\n",
    "        self.uidW = tf.Variable(tf.random.truncated_normal([self.user_field_M, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"uidW\")\n",
    "        self.iidW = tf.Variable(tf.random.truncated_normal([self.item_field_M+1, self.embedding_size],\n",
    "                                                             mean=0.0, stddev=0.01), name=\"iidW\")\n",
    "        self.H_i = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_i\")\n",
    "        self.H_s = tf.Variable(tf.constant(0.01, shape=[self.embedding_size, 1]), name=\"H_s\")\n",
    "        self.u_bias = tf.Variable(tf.random.truncated_normal([self.user_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"u_bias\")\n",
    "        self.i_bias = tf.Variable(tf.random.truncated_normal([self.item_field_M, 1],\n",
    "                                                               mean=0.0, stddev=0.01), name=\"i_bias\")\n",
    "        self.bias = tf.Variable(tf.constant(0.0), name=\"bias\")\n",
    "\n",
    "    def _create_vectors(self):\n",
    "        # User embedding\n",
    "        self.user_feature_emb = tf.nn.embedding_lookup(self.uidW, self.input_u)\n",
    "        self.summed_user_emb = tf.reduce_sum(self.user_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Apply dropout using TF1.x style (keep_prob)\n",
    "        self.H_i_drop = tf.compat.v1.nn.dropout(self.H_i, keep_prob=self.dropout_keep_prob)\n",
    "        self.H_s_drop = tf.compat.v1.nn.dropout(self.H_s, keep_prob=self.dropout_keep_prob)\n",
    "        # Item embedding\n",
    "        self.all_item_feature_emb = tf.nn.embedding_lookup(self.iidW, self.item_attribute)\n",
    "        self.summed_all_item_emb = tf.reduce_sum(self.all_item_feature_emb, axis=1)  # [batch, embed_size]\n",
    "        # Cross terms\n",
    "        self.user_cross = 0.5 * (tf.square(self.summed_user_emb) - tf.reduce_sum(tf.square(self.user_feature_emb), axis=1))\n",
    "        self.item_cross = 0.5 * (tf.square(self.summed_all_item_emb) - tf.reduce_sum(tf.square(self.all_item_feature_emb), axis=1))\n",
    "        # Compute scores (resulting in [batch, 1, 1])\n",
    "        self.user_cross_score = tf.matmul(tf.expand_dims(self.user_cross, 1), self.H_s_drop)\n",
    "        self.item_cross_score = tf.matmul(tf.expand_dims(self.item_cross, 1), self.H_s_drop)\n",
    "        # Bias terms\n",
    "        self.user_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.u_bias, self.input_u), axis=1)  # [batch, 1]\n",
    "        self.item_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.i_bias, self.item_attribute), axis=1)  # [batch, 1]\n",
    "        # Constant ones\n",
    "        self.I_user = tf.ones([tf.shape(self.input_u)[0], 1])\n",
    "        self.I_item = tf.ones([tf.shape(self.summed_all_item_emb)[0], 1])\n",
    "        # Instead of concatenating rank-3 tensors, squeeze the extra dims so that:\n",
    "        #   - summed_user_emb is [batch, embed_size]\n",
    "        #   - user_cross_score becomes [batch, 1]\n",
    "        #   - user_bias is [batch, 1]\n",
    "        # Then concatenate along axis 1 to get [batch, embed_size+2]\n",
    "        user_cross_score_squeezed = tf.squeeze(self.user_cross_score, axis=1)\n",
    "        user_bias_squeezed = tf.squeeze(tf.expand_dims(self.user_bias, 1), axis=1)\n",
    "        self.p_emb = tf.concat([self.summed_user_emb,\n",
    "                                 user_cross_score_squeezed + user_bias_squeezed + self.bias,\n",
    "                                 self.I_user],\n",
    "                                axis=1)\n",
    "        # Similarly for q_emb\n",
    "        item_cross_score_squeezed = tf.squeeze(self.item_cross_score, axis=1)\n",
    "        self.q_emb = tf.concat([self.summed_all_item_emb,\n",
    "                                 self.I_item,\n",
    "                                 item_cross_score_squeezed + tf.squeeze(tf.expand_dims(self.item_bias, 1), axis=1)],\n",
    "                                axis=1)\n",
    "        # Build H_i_emb by concatenating H_i_drop with two scalars so that its shape becomes [embed_size+2, 1]\n",
    "        self.H_i_emb = tf.concat([self.H_i_drop, [[1.0]], [[1.0]]], axis=0)\n",
    "\n",
    "    def _create_inference(self):\n",
    "        self.pos_item = tf.nn.embedding_lookup(self.q_emb, self.input_ur)\n",
    "        # Assumes that the global data object has attribute item_bind_M\n",
    "        self.pos_num_r = tf.cast(tf.not_equal(self.input_ur, data.item_bind_M), tf.float32)\n",
    "        self.pos_item = tf.einsum('ab,abc->abc', self.pos_num_r, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ac,abc->abc', self.p_emb, self.pos_item)\n",
    "        self.pos_r = tf.einsum('ajk,kl->ajl', self.pos_r, self.H_i_emb)\n",
    "        self.pos_r = tf.reshape(self.pos_r, [-1, self.max_item_pu])\n",
    "\n",
    "    def _pre(self):\n",
    "        dot = tf.einsum('ac,bc->abc', self.p_emb, self.q_emb)\n",
    "        pre = tf.einsum('ajk,kl->aj', dot, self.H_i_emb)\n",
    "        return pre\n",
    "\n",
    "    def _create_loss(self):\n",
    "        self.loss1 = self.weight1 * tf.reduce_sum(\n",
    "            tf.reduce_sum(tf.reduce_sum(tf.einsum('ab,ac->abc', self.q_emb, self.q_emb), axis=0) *\n",
    "                          tf.reduce_sum(tf.einsum('ab,ac->abc', self.p_emb, self.p_emb), axis=0) *\n",
    "                          tf.matmul(self.H_i_emb, self.H_i_emb, transpose_b=True), axis=0), axis=0)\n",
    "        self.loss1 += tf.reduce_sum((1.0 - self.weight1) * tf.square(self.pos_r) - 2.0 * self.pos_r)\n",
    "        self.l2_loss0 = tf.nn.l2_loss(self.uidW)\n",
    "        self.l2_loss1 = tf.nn.l2_loss(self.iidW)\n",
    "        self.loss = self.loss1 + self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "        self.reg_loss = self.lambda_bilinear[0] * self.l2_loss0 + self.lambda_bilinear[1] * self.l2_loss1\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._create_variables()\n",
    "        self._create_vectors()\n",
    "        self._create_inference()\n",
    "        self._create_loss()\n",
    "        self.pre = self._pre()\n",
    "\n",
    "def train_step(sess, deep, train_op, u_batch, i_batch, args):\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_ur: i_batch,\n",
    "        deep.dropout_keep_prob: args.dropout\n",
    "    }\n",
    "    _, loss, loss1, reg_loss = sess.run([train_op, deep.loss, deep.loss1, deep.reg_loss], feed_dict)\n",
    "    return loss, loss1, reg_loss\n",
    "\n",
    "def evaluate(sess, deep, data, args):\n",
    "    eva_batch = 128\n",
    "    recall_all = [[] for _ in range(len(args.topK))]\n",
    "    ndcg_all = [[] for _ in range(len(args.topK))]\n",
    "    user_features = data.user_test\n",
    "    num_batches = int(np.ceil(len(user_features) / eva_batch))\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * eva_batch\n",
    "        end_index = min((batch_num + 1) * eva_batch, len(user_features))\n",
    "        u_batch = user_features[start_index:end_index]\n",
    "        batch_users = end_index - start_index\n",
    "        feed_dict = { deep.input_u: u_batch, deep.dropout_keep_prob: 1.0 }\n",
    "        pre = sess.run(deep.pre, feed_dict)\n",
    "        pre = np.array(pre)\n",
    "        pre = np.delete(pre, -1, axis=1)\n",
    "        user_ids = [data.binded_users[\"-\".join(map(str, one))] for one in u_batch]\n",
    "        idx = np.zeros_like(pre, dtype=bool)\n",
    "        idx[data.Train_data[user_ids].nonzero()] = True\n",
    "        pre[idx] = -np.inf\n",
    "        for i, k in enumerate(args.topK):\n",
    "            idx_topk_part = np.argpartition(-pre, k, axis=1)[:, :k]\n",
    "            pre_bin = np.zeros_like(pre, dtype=bool)\n",
    "            pre_bin[np.arange(batch_users)[:, None], idx_topk_part] = True\n",
    "            true_bin = np.zeros_like(pre, dtype=bool)\n",
    "            true_bin[data.Test_data[user_ids].nonzero()] = True\n",
    "            hits = (np.logical_and(true_bin, pre_bin).sum(axis=1)).astype(np.float32)\n",
    "            recall_all[i].append(hits / np.minimum(k, true_bin.sum(axis=1)))\n",
    "            topk_part = pre[np.arange(batch_users)[:, None], idx_topk_part]\n",
    "            idx_part = np.argsort(-topk_part, axis=1)\n",
    "            idx_topk = idx_topk_part[np.arange(batch_users)[:, None], idx_part]\n",
    "            tp = np.log(2) / np.log(np.arange(2, k + 2))\n",
    "            test_batch = data.Test_data[user_ids]\n",
    "            DCG = (test_batch[np.arange(batch_users)[:, None], idx_topk].toarray() * tp).sum(axis=1)\n",
    "            IDCG = np.array([(tp[:min(n, k)]).sum() for n in test_batch.getnnz(axis=1)])\n",
    "            ndcg_all[i].append(DCG / IDCG)\n",
    "    recalls = [np.mean(np.hstack(r)) for r in recall_all]\n",
    "    ndcgs = [np.mean(np.hstack(n)) for n in ndcg_all]\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for i, k in enumerate(args.topK):\n",
    "        print(f\"Top {k}: HR={recalls[i]:.4f}, NDCG={ndcgs[i]:.4f}\")\n",
    "    return recalls, ndcgs\n",
    "\n",
    "def run_experiment(args, data, random_seed=2019):\n",
    "    with tf.Graph().as_default():\n",
    "        tf.compat.v1.set_random_seed(random_seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto()\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = ENSFM(data.item_map_list, data.user_field_M, data.item_field_M,\n",
    "                         args.embed_size, data.max_positive_len, args)\n",
    "            deep._build_graph()\n",
    "            train_op = tf.compat.v1.train.AdagradOptimizer(\n",
    "                learning_rate=args.lr, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            print(\"Initial evaluation:\")\n",
    "            evaluate(sess, deep, data, args)\n",
    "            for epoch in range(args.epochs):\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                shuffle_idx = np.random.permutation(len(data.user_train))\n",
    "                data.user_train = data.user_train[shuffle_idx]\n",
    "                data.item_train = data.item_train[shuffle_idx]\n",
    "                num_batches = int(np.ceil(len(data.user_train) / args.batch_size))\n",
    "                for batch_num in range(num_batches):\n",
    "                    start_index = batch_num * args.batch_size\n",
    "                    end_index = min((batch_num + 1) * args.batch_size, len(data.user_train))\n",
    "                    u_batch = data.user_train[start_index:end_index]\n",
    "                    i_batch = data.item_train[start_index:end_index]\n",
    "                    train_step(sess, deep, train_op, u_batch, i_batch, args)\n",
    "                if epoch % args.verbose == 0:\n",
    "                    evaluate(sess, deep, data, args)\n",
    "            print(\"Final evaluation:\")\n",
    "            recalls, ndcgs = evaluate(sess, deep, data, args)\n",
    "    hr_index = args.topK.index(10) if 10 in args.topK else 0\n",
    "    return recalls[hr_index], ndcgs[hr_index]\n",
    "\n",
    "# ==========================\n",
    "# Main Hyperparameter Search\n",
    "# ==========================\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    # Optionally update DATA_ROOT based on args.dataset\n",
    "    if args.dataset == 'lastfm':\n",
    "        print(\"Loading LastFM data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/lastfm'\n",
    "    elif args.dataset == 'frappe':\n",
    "        print(\"Loading Frappe data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/frappe'\n",
    "    elif args.dataset == 'ml-1m':\n",
    "        print(\"Loading ML-1M data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/ml-1m'\n",
    "    elif args.dataset == 'yelp2018':\n",
    "        print(\"Loading Yelp2018 data\")\n",
    "        DATA_ROOT = '/media/leo/Huy/Project/CARS/Yelp JSON/yelp_dataset/Shopping'\n",
    "    elif args.dataset == 'amazonbook':\n",
    "        print(\"Loading Amazon Book data\")\n",
    "        DATA_ROOT = '/content/drive/MyDrive/Record/data/amzbook'\n",
    "\n",
    "    # Load data\n",
    "    data = LoadData(DATA_ROOT)\n",
    "\n",
    "    # Define hyperparameter grid.\n",
    "    lr_values = [0.005, 0.01, 0.02, 0.05]\n",
    "    dropout_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "    neg_weight_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "    results = []\n",
    "    output_file = os.path.join(DATA_ROOT, 'ENSFM_hyperparam_results.txt')\n",
    "    with open(output_file, 'w') as f_out:\n",
    "        for lr, dropout, neg_weight in itertools.product(lr_values, dropout_values, neg_weight_values):\n",
    "            print(f\"\\nRunning experiment with lr={lr}, dropout={dropout}, negative_weight={neg_weight}\")\n",
    "            f_out.write(f\"Parameters: lr={lr}, dropout={dropout}, negative_weight={neg_weight}\\n\")\n",
    "            args.lr = lr\n",
    "            args.dropout = dropout\n",
    "            args.negative_weight = neg_weight\n",
    "            # Use fewer epochs for grid search\n",
    "            args.epochs = 50\n",
    "            hr, ndcg = run_experiment(args, data, random_seed=2019)\n",
    "            results.append(((lr, dropout, neg_weight), (hr, ndcg)))\n",
    "            f_out.write(f\"Final HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\\n\\n\")\n",
    "            f_out.flush()\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x[1][0], reverse=True)\n",
    "    print(\"\\nSorted hyperparameter search results (by HR@10):\")\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        f_out.write(\"Sorted hyperparameter search results (by HR@10):\\n\")\n",
    "        for combo, metrics in sorted_results:\n",
    "            line = f\"Parameters (lr, dropout, negative_weight): {combo} => HR@10: {metrics[0]:.4f}, NDCG@10: {metrics[1]:.4f}\\n\"\n",
    "            print(line.strip())\n",
    "            f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bcbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
